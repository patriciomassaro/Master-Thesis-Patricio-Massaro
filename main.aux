\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\citation{lst2005}
\citation{townshend94}
\citation{author2023thermal}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Thermal Remote Sensing}{3}{section.2}\protected@file@percent }
\newlabel{sec:thermal_remote_sensing}{{2}{3}{Thermal Remote Sensing}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Electromagnetic spectrum}{3}{subsection.2.1}\protected@file@percent }
\citation{sca2009}
\citation{mwa2001}
\citation{LI201314}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Electromagnetic spectrum\relax }}{4}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1-electromagnetic-spectrum}{{2.1}{4}{Electromagnetic spectrum\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Land surface temperature}{4}{subsection.2.2}\protected@file@percent }
\citation{becker90}
\citation{HORNING20082986}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Quality dimensions of remote sensing data}{5}{subsection.2.3}\protected@file@percent }
\citation{UNEP2021Wildfire}
\citation{lippitt2015time}
\citation{ijgi11120601}
\@writefile{toc}{\contentsline {section}{\numberline {3}Motivation}{7}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Wildfire Monitoring}{7}{subsection.3.1}\protected@file@percent }
\citation{atlanticcouncil2021extreme}
\citation{Hsu2021Disproportionate}
\citation{deilami2018urban}
\citation{mohamed2017land}
\citation{sobrino2012impact}
\citation{huang2013generating}
\citation{USGS2023Landsat}
\citation{terra_nasa}
\citation{Zhu2021}
\citation{Shi2019}
\citation{Shi2019}
\citation{Yang2017}
\citation{deilami2018urban}
\citation{Rouse1973MonitoringVS}
\citation{skopje2018}
\citation{Rouse1973MonitoringVS}
\citation{skopje2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Urban heat}{8}{subsection.3.2}\protected@file@percent }
\citation{rs11050573}
\citation{rs13081524}
\citation{rs12182949}
\newlabel{fig:1-skopje-NDVI}{{\caption@xref {fig:1-skopje-NDVI}{ on input line 224}}{9}{Urban heat}{figure.caption.4}{}}
\newlabel{fig:1-skopje-LST}{{\caption@xref {fig:1-skopje-LST}{ on input line 229}}{9}{Urban heat}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Normalized Difference Vegetation Index \cite  {Rouse1973MonitoringVS} and LST measurements for the zone of Skopje, North Macedonia. Urban areas with a lower vegetation index tend to have a higher temperature than their rural counterparts (Source : \cite  {skopje2018}).\relax }}{9}{figure.caption.4}\protected@file@percent }
\newlabel{fig:1-skopie-UHI}{{3.1}{9}{Normalized Difference Vegetation Index \cite {Rouse1973MonitoringVS} and LST measurements for the zone of Skopje, North Macedonia. Urban areas with a lower vegetation index tend to have a higher temperature than their rural counterparts (Source : \cite {skopje2018}).\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Irrigation Management}{9}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}The spatio-temporal trade off}{10}{subsection.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Scatter plot of the spatial and temporal resolution of some of the LST/TIR data products available. The trade-off is evident, no mission can provide products in the zone of interest. Constellations may help with temporal resolution, but the spatial resolution is still limited.\relax }}{11}{figure.caption.5}\protected@file@percent }
\newlabel{fig:1-spatio-temporal-trade-off}{{3.2}{11}{Scatter plot of the spatial and temporal resolution of some of the LST/TIR data products available. The trade-off is evident, no mission can provide products in the zone of interest. Constellations may help with temporal resolution, but the spatial resolution is still limited.\relax }{figure.caption.5}{}}
\citation{zeyde2010single}
\citation{martin2001database}
\citation{valsesia2021permutation}
\citation{bashir2021comprehensive}
\@writefile{toc}{\contentsline {section}{\numberline {4}Super resolution}{12}{section.4}\protected@file@percent }
\newlabel{sec:SR}{{4}{12}{Super resolution}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Example of super resolution as an ill posed problem. A blurry picture of Barack Obama can be generated from an HR image of another person.\relax }}{12}{figure.caption.6}\protected@file@percent }
\newlabel{fig:2-SR-ill-posed}{{4.1}{12}{Example of super resolution as an ill posed problem. A blurry picture of Barack Obama can be generated from an HR image of another person.\relax }{figure.caption.6}{}}
\citation{bashir2021comprehensive}
\citation{bashir2021comprehensive}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces In traditional machine learning, the feature extraction step is crucial for performance, requiring domain knowledge. In deep learning, the feature extraction is learned from the data.\relax }}{13}{figure.caption.7}\protected@file@percent }
\newlabel{fig:2-end-to-end-training}{{4.2}{13}{In traditional machine learning, the feature extraction step is crucial for performance, requiring domain knowledge. In deep learning, the feature extraction is learned from the data.\relax }{figure.caption.7}{}}
\citation{Liu2019}
\citation{Liu2019}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Example of generating a super resolution dataset using a simplified known degradation model (Source: \cite  {bashir2021comprehensive}).\relax }}{14}{figure.caption.8}\protected@file@percent }
\newlabel{fig:3-super-resolution-data}{{4.3}{14}{Example of generating a super resolution dataset using a simplified known degradation model (Source: \cite {bashir2021comprehensive}).\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Single-Image Super Resolution}{14}{subsection.4.1}\protected@file@percent }
\newlabel{eq:2-degradation-equation}{{4}{14}{Single-Image Super Resolution}{equation.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Upsampling method}{14}{subsubsection.4.1.1}\protected@file@percent }
\citation{timofte2015seven}
\citation{lai2017deep}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Sub-pixel convolution schematic (Source: \cite  {Liu2019}).\relax }}{15}{figure.caption.9}\protected@file@percent }
\newlabel{fig:2-sub-pixel-convolution}{{4.4}{15}{Sub-pixel convolution schematic (Source: \cite {Liu2019}).\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Network design}{15}{subsubsection.4.1.2}\protected@file@percent }
\newlabel{eq:2-residual-learning}{{5}{15}{Network design}{equation.4.5}{}}
\citation{Dai2019}
\citation{zhang2018image}
\citation{zhang2018image}
\citation{simonyan2015deep}
\citation{he2015deep}
\citation{ledig2017photorealistic}
\citation{wang2018recovering}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Residual Channel attention schematic. This approach combines both residual learning and channel attention (Source: \cite  {zhang2018image}).\relax }}{16}{figure.caption.10}\protected@file@percent }
\newlabel{fig:2-feature-attention}{{4.5}{16}{Residual Channel attention schematic. This approach combines both residual learning and channel attention (Source: \cite {zhang2018image}).\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Loss functions}{16}{subsubsection.4.1.3}\protected@file@percent }
\citation{goodfellow2014generative}
\citation{mao2017squares}
\citation{ledig2017photorealistic}
\citation{ledig2017photorealistic}
\citation{martens2019superresolution}
\citation{Salvetti_2020}
\citation{Bordone_Molini_2020}
\citation{MISR2007}
\citation{MISR2007}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Illustration of patches from the natural image manifold and results coming from MSE pixel-loss (red) and GANs (orange) (source:\cite  {ledig2017photorealistic}).\relax }}{18}{figure.caption.11}\protected@file@percent }
\newlabel{fig:2-gans-natural-manifold}{{4.6}{18}{Illustration of patches from the natural image manifold and results coming from MSE pixel-loss (red) and GANs (orange) (source:\cite {ledig2017photorealistic}).\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Multi-Image Super Resolution}{18}{subsection.4.2}\protected@file@percent }
\citation{myself2023}
\citation{myself2023}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Multi-image super resolution algorithms combine multiple low-resolution image acquisitions into a high-resolution image (Source: \cite  {MISR2007}).\relax }}{19}{figure.caption.12}\protected@file@percent }
\newlabel{fig:2-MISR}{{4.7}{19}{Multi-image super resolution algorithms combine multiple low-resolution image acquisitions into a high-resolution image (Source: \cite {MISR2007}).\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Multi-spectral super resolution}{19}{subsubsection.4.2.1}\protected@file@percent }
\citation{myself2023}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Example of Pan-sharpening on TIR data using a panchromatic band. (a) Panchromatic band, (b) HR TIR image, (c) Downsampled version of the TIR image , (d) Pan-sharpened image. The pan-sharpened image is less blurry than the LR, but a lot of artifacts are produced, specially in clouds (Source: \cite  {myself2023}).\relax }}{20}{figure.caption.13}\protected@file@percent }
\newlabel{fig:2-pansharpening}{{4.8}{20}{Example of Pan-sharpening on TIR data using a panchromatic band. (a) Panchromatic band, (b) HR TIR image, (c) Downsampled version of the TIR image , (d) Pan-sharpened image. The pan-sharpened image is less blurry than the LR, but a lot of artifacts are produced, specially in clouds (Source: \cite {myself2023}).\relax }{figure.caption.13}{}}
\citation{lugmayr2020ntire}
\citation{liu2021blind}
\citation{liu2021blind}
\citation{accurateblurs2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}The domain gap problem}{21}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:domaingap}{{4.3}{21}{The domain gap problem}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Effects of different degradation models on one HR image (Source: \cite  {liu2021blind}).\relax }}{21}{figure.caption.14}\protected@file@percent }
\newlabel{fig:2-domain-gap}{{4.9}{21}{Effects of different degradation models on one HR image (Source: \cite {liu2021blind}).\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Blind image Super Resolution}{21}{subsection.4.4}\protected@file@percent }
\citation{liu2021blind}
\citation{liu2021blind}
\citation{zhang2018residual}
\citation{zhang2018residual}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Domain interpretation of differences between non-blind and blind SR (Source: \cite  {liu2021blind}).\relax }}{22}{figure.caption.15}\protected@file@percent }
\newlabel{fig:2-DomainGap}{{4.10}{22}{Domain interpretation of differences between non-blind and blind SR (Source: \cite {liu2021blind}).\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Explicit modelling with external dataset}{22}{subsubsection.4.4.1}\protected@file@percent }
\citation{gu2019blind}
\citation{luo2020unfolding}
\citation{zhou19}
\citation{zontak2011}
\citation{glasner2009}
\citation{bellkligler2020blind}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Dimensionality stretching strategy to concatenate the degradation map to the LR input. The vectorized kernel is projected onto a space of a lower dimensionality, and then stretched to generate $t$ feature maps with the same shape of the input image. The noise level is also concatenated (Source: \cite  {zhang2018residual}).\relax }}{23}{figure.caption.16}\protected@file@percent }
\newlabel{fig:2-external-dataset-stretching}{{4.11}{23}{Dimensionality stretching strategy to concatenate the degradation map to the LR input. The vectorized kernel is projected onto a space of a lower dimensionality, and then stretched to generate $t$ feature maps with the same shape of the input image. The noise level is also concatenated (Source: \cite {zhang2018residual}).\relax }{figure.caption.16}{}}
\citation{bellkligler2020blind}
\citation{bellkligler2020blind}
\citation{shocher2017zeroshot}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Explicit modelling with single image}{24}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces KernelGAN schematic diagram. The discriminator tries to distinguish between the generated patches and the original LR image patches. G learns perform 2x downscaling while fooling the discriminator by maintaining the same distribution of patches (Source \cite  {bellkligler2020blind}).\relax }}{24}{figure.caption.17}\protected@file@percent }
\newlabel{fig:2-kernelGAN}{{4.12}{24}{KernelGAN schematic diagram. The discriminator tries to distinguish between the generated patches and the original LR image patches. G learns perform 2x downscaling while fooling the discriminator by maintaining the same distribution of patches (Source \cite {bellkligler2020blind}).\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Implicit modelling}{24}{subsubsection.4.4.3}\protected@file@percent }
\newlabel{subsubsec:implicit-modelling}{{4.4.3}{24}{Implicit modelling}{subsubsection.4.4.3}{}}
\citation{CycleGAN2017}
\citation{yuan2018unsupervised}
\citation{yuan2018unsupervised}
\citation{yuan2018unsupervised}
\citation{luo2022learning}
\citation{bulat2018learn}
\citation{luo2022learning}
\citation{bulat2018learn}
\citation{wei2020unsupervised}
\citation{bulat2018learn}
\citation{bulat2018learn}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces CinCGAN schematic diagram (Source: \cite  {yuan2018unsupervised}).\relax }}{25}{figure.caption.18}\protected@file@percent }
\newlabel{fig:2-CinCGAN}{{4.13}{25}{CinCGAN schematic diagram (Source: \cite {yuan2018unsupervised}).\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Degradation GAN schematic diagram. The architecture includes one LR generator, on SR network and two discriminators (Source: \cite  {bulat2018learn}).\relax }}{26}{figure.caption.19}\protected@file@percent }
\newlabel{fig:2-degradation-gan}{{4.14}{26}{Degradation GAN schematic diagram. The architecture includes one LR generator, on SR network and two discriminators (Source: \cite {bulat2018learn}).\relax }{figure.caption.19}{}}
\citation{luo2022learning}
\citation{zhu2020unpaired}
\@writefile{toc}{\contentsline {section}{\numberline {5}Methodology}{27}{section.5}\protected@file@percent }
\newlabel{sec:methodology}{{5}{27}{Methodology}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Models Architecture}{27}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Probabilistic degradation model}{27}{subsubsection.5.1.1}\protected@file@percent }
\citation{plotz2017benchmarking}
\citation{luo2022learning}
\citation{luo2022learning}
\citation{bulat2018learn}
\citation{wei2020unsupervised}
\citation{bulat2018learn}
\citation{isola2018imagetoimage}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Schematic of the probabilistic degradation module. The discriminator is left out for a more intuitive description. Source: \cite  {luo2022learning}\relax }}{29}{figure.caption.20}\protected@file@percent }
\newlabel{fig:3-probabilistic-degradation-model}{{5.1}{29}{Schematic of the probabilistic degradation module. The discriminator is left out for a more intuitive description. Source: \cite {luo2022learning}\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Schematic of the generative networks used in the kernel and noise module of the probabilistic degradation model. The parameters of the convolutional layers represent input channels, output channels, kernel size, stride and padding, respectively. The residual blocks use the same kernel size as the convolutional layers of each module. In the noise module, the random vector $z_n$ is concatenated with $I_{\text  {clean}}^{\text  {LR}}$ before the first convolutional layer. \relax }}{29}{figure.caption.21}\protected@file@percent }
\newlabel{fig:3-slim-gen-module}{{5.2}{29}{Schematic of the generative networks used in the kernel and noise module of the probabilistic degradation model. The parameters of the convolutional layers represent input channels, output channels, kernel size, stride and padding, respectively. The residual blocks use the same kernel size as the convolutional layers of each module. In the noise module, the random vector $z_n$ is concatenated with $I_{\text {clean}}^{\text {LR}}$ before the first convolutional layer. \relax }{figure.caption.21}{}}
\citation{fritsche2019frequency}
\citation{wei2020unsupervised}
\citation{luo2022learning}
\citation{luo2022learning}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Diagram of the PatchGAN discriminator. The parameters of the convolutional layers represent input channels, output channels, kernel size, stride and padding, respectively. \relax }}{30}{figure.caption.22}\protected@file@percent }
\newlabel{fig:3-slim-patchgan-module}{{5.3}{30}{Diagram of the PatchGAN discriminator. The parameters of the convolutional layers represent input channels, output channels, kernel size, stride and padding, respectively. \relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The probabilistic degradation model is used to encourage the degradation model to produce images in the same domain as the test LR images. After training, the SR model is directly used to super resolve the inputs. Source \cite  {luo2022learning}.\relax }}{30}{figure.caption.23}\protected@file@percent }
\newlabel{fig:3-GAN-degradation-model}{{5.4}{30}{The probabilistic degradation model is used to encourage the degradation model to produce images in the same domain as the test LR images. After training, the SR model is directly used to super resolve the inputs. Source \cite {luo2022learning}.\relax }{figure.caption.23}{}}
\citation{myself2023}
\citation{ledig2017photorealistic}
\citation{simonyan2015deep}
\citation{zhang2018residual}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}SRResNet}{31}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces  Modified SRResNet architecture. $X_{LR}$ represents the low resolution input image, $X_{SR}$ the super resolved image, which is then compared to the ground truth $X_{HR}$.\relax }}{31}{figure.caption.24}\protected@file@percent }
\newlabel{fig:3-resnet-architecture}{{5.5}{31}{Modified SRResNet architecture. $X_{LR}$ represents the low resolution input image, $X_{SR}$ the super resolved image, which is then compared to the ground truth $X_{HR}$.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Baseline Degradation model}{31}{subsection.5.2}\protected@file@percent }
\newlabel{subsec:baseline_degradation_model}{{5.2}{31}{Baseline Degradation model}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Blurring Kernel}{32}{subsubsection.5.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Example of kernels used in a stochastic degradation model. (a),(b) and (c) are generated using a symmetric variance on the x and y axis. (d) (e) and (f) are generated using an asymmetric variances, resulting in much more anisotropic kernels.\relax }}{33}{figure.caption.25}\protected@file@percent }
\newlabel{fig:4-degradation_kernels}{{5.6}{33}{Example of kernels used in a stochastic degradation model. (a),(b) and (c) are generated using a symmetric variance on the x and y axis. (d) (e) and (f) are generated using an asymmetric variances, resulting in much more anisotropic kernels.\relax }{figure.caption.25}{}}
\citation{ECOSTRESS2023INSTRUMENT}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Effects of different blurring kernels on the HR-LR generation. The upper row contains images generated using blurring kernels with symmetric distributions. The lower rows contains images generated using asymmetric distributions, resulting in highly anisotropic kernels.\relax }}{34}{figure.caption.26}\protected@file@percent }
\newlabel{fig:4-degradation-kernel-examples}{{5.7}{34}{Effects of different blurring kernels on the HR-LR generation. The upper row contains images generated using blurring kernels with symmetric distributions. The lower rows contains images generated using asymmetric distributions, resulting in highly anisotropic kernels.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Radiometric error correction}{34}{subsubsection.5.2.2}\protected@file@percent }
\newlabel{eq:4-radiometric-error-correction}{{17}{34}{Radiometric error correction}{equation.5.17}{}}
\newlabel{eq:4-planck-derivative}{{18}{34}{Radiometric error correction}{equation.5.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Effects of increasing radiometric error on the HR-LR generation.\relax }}{35}{figure.caption.27}\protected@file@percent }
\newlabel{fig:4-radiometric_noise_example}{{5.8}{35}{Effects of increasing radiometric error on the HR-LR generation.\relax }{figure.caption.27}{}}
\citation{mao2017squares}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Signal-to-Noise Ratio (SNR)}{36}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Referenced image quality metrics}{36}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}Pixel-wise Losses}{36}{subsubsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}Adversarial loss}{36}{subsubsection.5.4.2}\protected@file@percent }
\citation{VGGnet}
\citation{zhang2018unreasonable}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3}Peak Signal-to-Noise Ratio (PSNR)}{37}{subsubsection.5.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.4}Structural Similarity Index (SSIM)}{37}{subsubsection.5.4.4}\protected@file@percent }
\citation{martens2019superresolution}
\citation{niqe}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.5}Learned Perceptual Image Patch Similarity (LPIPS)}{38}{subsubsection.5.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.6}Adjusting measures to bias and translations during the SR process.}{38}{subsubsection.5.4.6}\protected@file@percent }
\newlabel{subsec:adjustedmetrics}{{5.4.6}{38}{Adjusting measures to bias and translations during the SR process}{subsubsection.5.4.6}{}}
\newlabel{eq:4_adjusted_metrics}{{24}{38}{Adjusting measures to bias and translations during the SR process}{equation.5.24}{}}
\citation{niqe}
\citation{mittal2012}
\citation{fuoli2021fourier}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Non-referenced Image quality metrics}{39}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.1}Naturalness Image Quality Evaluator (NIQE)}{39}{subsubsection.5.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2}Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE)}{39}{subsubsection.5.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.3}Frequency Domain Analysis}{40}{subsubsection.5.5.3}\protected@file@percent }
\newlabel{subsubsec:frequency_domain_analysis}{{5.5.3}{40}{Frequency Domain Analysis}{subsubsection.5.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Comparison between (a) the radial representation of frequencies in cycles per pixel and (b) the frequency normalized by the direction-dependent Nyquist limit. The frequency limit is always on the borders of the FFT plot, but they represent different frequency values depending on the direction. \relax }}{41}{figure.caption.28}\protected@file@percent }
\newlabel{fig:5-square_vs_radial}{{5.9}{41}{Comparison between (a) the radial representation of frequencies in cycles per pixel and (b) the frequency normalized by the direction-dependent Nyquist limit. The frequency limit is always on the borders of the FFT plot, but they represent different frequency values depending on the direction. \relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Steps of the frequency domain analysis. (b) shows the log magnitude of the shifted FFT of the scene depicted in (a). The radial profile is calculating by averaging all the points that have the same $r$. In (c), the log magnitude obtained for every radial profile is plotted, translating the axis from radius into spatial frequency.\relax }}{41}{figure.caption.29}\protected@file@percent }
\newlabel{fig:4-frequency-analysis}{{5.10}{41}{Steps of the frequency domain analysis. (b) shows the log magnitude of the shifted FFT of the scene depicted in (a). The radial profile is calculating by averaging all the points that have the same $r$. In (c), the log magnitude obtained for every radial profile is plotted, translating the axis from radius into spatial frequency.\relax }{figure.caption.29}{}}
\citation{Sobel1990AnI3}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.4}Gradient Distribution analysis}{42}{subsubsection.5.5.4}\protected@file@percent }
\newlabel{eq:4-sobel-operators}{{27}{42}{Gradient Distribution analysis}{equation.5.27}{}}
\newlabel{eq:4-gradient_magnitude}{{28}{42}{Gradient Distribution analysis}{equation.5.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Steps to obtain a gradient magnitude density. Using the sobel operators, $G_x$ and $G_y$ are obtained from an image. The magnitude $|G|$ of each pixel is calculated using Eq. \ref {eq:4-gradient_magnitude}. The density can be estimated afterwards, using 100 bins in this case.\relax }}{43}{figure.caption.30}\protected@file@percent }
\newlabel{fig:4-gradient-analysis}{{5.11}{43}{Steps to obtain a gradient magnitude density. Using the sobel operators, $G_x$ and $G_y$ are obtained from an image. The magnitude $|G|$ of each pixel is calculated using Eq. \ref {eq:4-gradient_magnitude}. The density can be estimated afterwards, using 100 bins in this case.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.5}Correlation between pixel and neighbors }{43}{subsubsection.5.5.5}\protected@file@percent }
\newlabel{eq:4-sobel-operators}{{29}{43}{Correlation between pixel and neighbors}{equation.5.29}{}}
\citation{ECOSTRESS2023}
\citation{PhyTIR2023}
\citation{ECOSTRESS2023}
\@writefile{toc}{\contentsline {section}{\numberline {6}Datasets}{44}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Obtaining a high resolution dataset}{44}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}The ECOSTRESS mission}{44}{subsubsection.6.1.1}\protected@file@percent }
\citation{AppEEARS2023}
\citation{AppEEARSAPI2023}
\citation{ECO1BMAPRAD2023}
\citation{ecostress_faq}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Wavelengths of the sensors in ECOSTRESS and FOREST satellites. The radiation spectrum of black-bodies at different temperatures are included for comparison.\relax }}{45}{figure.caption.31}\protected@file@percent }
\newlabel{fig:5-wavelength-comparison}{{6.1}{45}{Wavelengths of the sensors in ECOSTRESS and FOREST satellites. The radiation spectrum of black-bodies at different temperatures are included for comparison.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Accessing ECOSTRESS Scenes}{45}{subsubsection.6.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Requests configuration\relax }}{45}{table.caption.32}\protected@file@percent }
\newlabel{tab:5-scenes-characteristics}{{6.1}{45}{Requests configuration\relax }{table.caption.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Selecting the best scenes}{45}{subsubsection.6.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Location of the samples taken from ecostress.\relax }}{46}{figure.caption.33}\protected@file@percent }
\newlabel{fig:5-ecostress-map-location}{{6.2}{46}{Location of the samples taken from ecostress.\relax }{figure.caption.33}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Process applied to the scenes returned from one area request.\relax }}{46}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.4}Data Processing}{47}{subsubsection.6.1.4}\protected@file@percent }
\newlabel{tab:quality_classes}{{\caption@xref {tab:quality_classes}{ on input line 113}}{47}{Data Processing}{table.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Fill Value and Data Quality Classes\relax }}{47}{table.caption.34}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Data processing workflow\relax }}{48}{figure.caption.35}\protected@file@percent }
\newlabel{fig:5-data_processing_flow_chart}{{6.3}{48}{Data processing workflow\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Random crop processor\relax }}{48}{figure.caption.36}\protected@file@percent }
\newlabel{fig:5-random_crop_processor}{{6.4}{48}{Random crop processor\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Obtaining FOREST-2 data}{48}{subsection.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Location of the FOREST-2 scenes.\relax }}{49}{figure.caption.37}\protected@file@percent }
\newlabel{fig:4-forest-locations}{{6.5}{49}{Location of the FOREST-2 scenes.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces LWIR1, LWIR2 and MWIR bands of a FOREST-2 scene downloaded from the company's API.\relax }}{49}{figure.caption.38}\protected@file@percent }
\newlabel{fig:4-forest-complete example}{{6.6}{49}{LWIR1, LWIR2 and MWIR bands of a FOREST-2 scene downloaded from the company's API.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces LWIR1 and LWIR2 of a FOREST-2 scene downloaded from the company's API, after cropping NA values.\relax }}{50}{figure.caption.39}\protected@file@percent }
\newlabel{fig:4-forest-bounding-box}{{6.7}{50}{LWIR1 and LWIR2 of a FOREST-2 scene downloaded from the company's API, after cropping NA values.\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Datasets}{50}{subsection.6.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Dataset characteristics\relax }}{50}{table.caption.40}\protected@file@percent }
\newlabel{tab:dataset_characteristics}{{6.3}{50}{Dataset characteristics\relax }{table.caption.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Synthetic FOREST - Degraded Synthetic FOREST}{50}{subsubsection.6.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Parameters used in the degradation model employed to generate the $\mathcal  {D}_{\text  {SF}-\text  {SF}}$ dataset.\relax }}{51}{table.caption.41}\protected@file@percent }
\newlabel{tab:degradation_model_parameters}{{6.4}{51}{Parameters used in the degradation model employed to generate the $\mathcal {D}_{\text {SF}-\text {SF}}$ dataset.\relax }{table.caption.41}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Synthetic FOREST - real FOREST (Unpaired)}{51}{subsubsection.6.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Experiment Setup}{52}{section.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Experiment setup parameters\relax }}{53}{table.caption.42}\protected@file@percent }
\newlabel{tab:experiment-setup}{{7.1}{53}{Experiment setup parameters\relax }{table.caption.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Results and discussion}{54}{section.8}\protected@file@percent }
\newlabel{sec:results}{{8}{54}{Results and discussion}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Source domain}{54}{subsection.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces \small  {\small  {Applying degradation models on an HR sample. The 2 most upper rows show the estimated kernels and noise for each pipeline (bicubic downsampling does not perform any estimation). The degraded images from each pipeline are displayed afterwards. The PSNR is calculated against the bicubic downsampling LR. The SR results are displayed in the last 2 rows. The PSNR for each SR method is calculated against the ground truth.}} \relax }}{55}{figure.caption.43}\protected@file@percent }
\newlabel{fig:5-source_domain_sample}{{8.1}{55}{\small {\small {Applying degradation models on an HR sample. The 2 most upper rows show the estimated kernels and noise for each pipeline (bicubic downsampling does not perform any estimation). The degraded images from each pipeline are displayed afterwards. The PSNR is calculated against the bicubic downsampling LR. The SR results are displayed in the last 2 rows. The PSNR for each SR method is calculated against the ground truth.}} \relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Log mangnitude of the FFT for the LR images obtained by the pipelines and the gaussian blurring + bicubic upsampling.\relax }}{56}{figure.caption.44}\protected@file@percent }
\newlabel{fig:5-lr-images-fft.pdf}{{8.2}{56}{Log mangnitude of the FFT for the LR images obtained by the pipelines and the gaussian blurring + bicubic upsampling.\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces (a) Radial profile of the log magnitude across spatial frequency of the LR images obtained by the pipelines and the gaussian blurring + bicubic downsampling model. (b) Amplification in dB of each pipeline with respect to the gaussian blurring + bicubic downsampling.\relax }}{57}{figure.caption.45}\protected@file@percent }
\newlabel{fig:5-lr-images-fft-comparison.pdf}{{8.3}{57}{(a) Radial profile of the log magnitude across spatial frequency of the LR images obtained by the pipelines and the gaussian blurring + bicubic downsampling model. (b) Amplification in dB of each pipeline with respect to the gaussian blurring + bicubic downsampling.\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Frequency domain analysis of the SR images and the ground truth displayed in Fig. \ref {fig:5-source_domain_sample}. In (a), the log of the magnitude of the FFT for the SR images and the ground truth is shown, while in (b), the amplification of each SR image with respect to the ground truth is shown.\relax }}{58}{figure.caption.46}\protected@file@percent }
\newlabel{fig:5-source-sr-fft-comparison}{{8.4}{58}{Frequency domain analysis of the SR images and the ground truth displayed in Fig. \ref {fig:5-source_domain_sample}. In (a), the log of the magnitude of the FFT for the SR images and the ground truth is shown, while in (b), the amplification of each SR image with respect to the ground truth is shown.\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}Probabilistic degradation models comparison}{58}{subsubsection.8.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Mean and standard deviation of the estimated kernels for the baseline and adapted degradation model, using 2000 realizations of $z_k$. The standard deviation of each pixel is normalized by the corresponding mean value. Kernel pixels with mean lower than $10^{-4}$ are considered with 0 std for clarity in the plot.\relax }}{59}{figure.caption.47}\protected@file@percent }
\newlabel{fig:5-source-kernel-mean-std}{{8.5}{59}{Mean and standard deviation of the estimated kernels for the baseline and adapted degradation model, using 2000 realizations of $z_k$. The standard deviation of each pixel is normalized by the corresponding mean value. Kernel pixels with mean lower than $10^{-4}$ are considered with 0 std for clarity in the plot.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Distribution of SNR values using $I^{\text  {LR}}_{\text  {clean}}$, product of the convolution of the kernel and $I^{\text  {HR}}_{\text  {clean}}$, and the noise module output for both pipelines. The output noise is generated 2000 times, using different realizations of the random variable $z_n$ for each iteration and the same input image. \relax }}{60}{figure.caption.48}\protected@file@percent }
\newlabel{fig:5-source-noise-1-sample}{{8.6}{60}{Distribution of SNR values using $I^{\text {LR}}_{\text {clean}}$, product of the convolution of the kernel and $I^{\text {HR}}_{\text {clean}}$, and the noise module output for both pipelines. The output noise is generated 2000 times, using different realizations of the random variable $z_n$ for each iteration and the same input image. \relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces Comparison of the SNR expressed in dB of the low resolution images generated by the baseline and adapted degradation model.\relax }}{61}{figure.caption.49}\protected@file@percent }
\newlabel{fig:5-source-noise-SNR}{{8.7}{61}{Comparison of the SNR expressed in dB of the low resolution images generated by the baseline and adapted degradation model.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces Comparison of the Pearson correlation coefficient between $I^{\text  {LR}}_{\text  {clean}}$ and the output of the noise module for the baseline and adapted degradation model.\relax }}{61}{figure.caption.50}\protected@file@percent }
\newlabel{fig:5-source-noise-correlation}{{8.8}{61}{Comparison of the Pearson correlation coefficient between $I^{\text {LR}}_{\text {clean}}$ and the output of the noise module for the baseline and adapted degradation model.\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Low resolution images comparison}{62}{subsubsection.8.1.2}\protected@file@percent }
\newlabel{subsec:results-lr-comparison}{{8.1.2}{62}{Low resolution images comparison}{subsubsection.8.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces Performance metrics between the LR images obtained by the pipelines vs the gaussian blurring + bicubic downsampling degradation. On the left, the PSNR is displayed. On the middle and the right, SSIM and LPIPS are represented respectively.\relax }}{62}{figure.caption.51}\protected@file@percent }
\newlabel{fig:5-source-domain-lr-performance-scatterplot}{{8.9}{62}{Performance metrics between the LR images obtained by the pipelines vs the gaussian blurring + bicubic downsampling degradation. On the left, the PSNR is displayed. On the middle and the right, SSIM and LPIPS are represented respectively.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces Frequency domain analysis of the LR images obtained by applying different degradation models on the HR sample displayed in Fig. \ref {fig:5-source_domain_sample}. In (a), the log of the magnitude of the FFT for the LR images is shown, while in (b), the amplification with respect to a simple gaussian blurring + downscaling is shown. The painted area represents the ±1 standard deviation of the radial profiles and the amplification. \relax }}{63}{figure.caption.52}\protected@file@percent }
\newlabel{fig:5-lr-images-fft-comparison}{{8.10}{63}{Frequency domain analysis of the LR images obtained by applying different degradation models on the HR sample displayed in Fig. \ref {fig:5-source_domain_sample}. In (a), the log of the magnitude of the FFT for the LR images is shown, while in (b), the amplification with respect to a simple gaussian blurring + downscaling is shown. The painted area represents the ±1 standard deviation of the radial profiles and the amplification. \relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.3}Effects of the degradation model in super resolution performance}{63}{subsubsection.8.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces Performance obtained by super resolving the degraded images coming out of the generator. In (a), the corresponding SR model of each pipeline is used. In (b), a simple bicubic upsampling is used to super resolve the degraded images instead of the SR model. The Pearson correlation coefficient is represented by $\rho $. \relax }}{64}{figure.caption.53}\protected@file@percent }
\newlabel{fig:5-source-domain-comparison}{{8.11}{64}{Performance obtained by super resolving the degraded images coming out of the generator. In (a), the corresponding SR model of each pipeline is used. In (b), a simple bicubic upsampling is used to super resolve the degraded images instead of the SR model. The Pearson correlation coefficient is represented by $\rho $. \relax }{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Target domain}{64}{subsection.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces Super Resolved Forest-2 Scene using different SR models. In the upper row, the image is displayed. A detailed zoom is displayed below. The original image is displayed in the left, while the super resolved images are displayed afterwards. \relax }}{65}{figure.caption.54}\protected@file@percent }
\newlabel{fig:5-target_prediction_sample}{{8.12}{65}{Super Resolved Forest-2 Scene using different SR models. In the upper row, the image is displayed. A detailed zoom is displayed below. The original image is displayed in the left, while the super resolved images are displayed afterwards. \relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces Frequency domain analysis of the SR images obtained by applying different SR models to the real FOREST-2 validation dataset. In (a), the log of the magnitude of the FFT for the SR images is shown, while in (b) the amplification with respect to a simple bicubic upsampling is displayed.\relax }}{66}{figure.caption.55}\protected@file@percent }
\newlabel{fig:5-target-amplification-statistics}{{8.13}{66}{Frequency domain analysis of the SR images obtained by applying different SR models to the real FOREST-2 validation dataset. In (a), the log of the magnitude of the FFT for the SR images is shown, while in (b) the amplification with respect to a simple bicubic upsampling is displayed.\relax }{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.14}{\ignorespaces Gradient analysis of the super resolved images using different SR models for scenes coming from the real FOREST-2 validation dataset. In the upper row, the image is displayed. The gradients in the x and y direction ($G_x$ and $G_y$ respectiely) are displayed below. the gradient magnitude $|G|$ is displayed in the bottom row.\relax }}{67}{figure.caption.56}\protected@file@percent }
\newlabel{fig:6-target-gradient-analysis-image}{{8.14}{67}{Gradient analysis of the super resolved images using different SR models for scenes coming from the real FOREST-2 validation dataset. In the upper row, the image is displayed. The gradients in the x and y direction ($G_x$ and $G_y$ respectiely) are displayed below. the gradient magnitude $|G|$ is displayed in the bottom row.\relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.15}{\ignorespaces Density estimation of the gradient magnitude $|G|$ for the super resolved real FOREST-2 images from the validation dataset. The magnitudes of the Synthetic HR FOREST-2 images are also computed for comparison.\relax }}{68}{figure.caption.57}\protected@file@percent }
\newlabel{fig:5-gradient-histogram-validation-dataset}{{8.15}{68}{Density estimation of the gradient magnitude $|G|$ for the super resolved real FOREST-2 images from the validation dataset. The magnitudes of the Synthetic HR FOREST-2 images are also computed for comparison.\relax }{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.16}{\ignorespaces Density estimation of the correlation coefficient between the pixels and their neighborhoods, for the super resolved real FOREST-2 images from the validation dataset. The coefficients of the synthetic HR FOREST-2 images are also computed for comparison.\relax }}{68}{figure.caption.58}\protected@file@percent }
\newlabel{fig:5-correlation-histogram-validation-dataset}{{8.16}{68}{Density estimation of the correlation coefficient between the pixels and their neighborhoods, for the super resolved real FOREST-2 images from the validation dataset. The coefficients of the synthetic HR FOREST-2 images are also computed for comparison.\relax }{figure.caption.58}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Sensibility to domain gap}{69}{subsection.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.17}{\ignorespaces Effects of using a model trained on a different domain than at inference time. When using an Synthetic FOREST image degraded with the baseline degradation model as an input, the model trained using real FOREST-2 data as the target domain generates several artifacts and underperforms severely in terms of PSNR. \relax }}{69}{figure.caption.59}\protected@file@percent }
\newlabel{fig:5-target-prediction-with-domain-gap}{{8.17}{69}{Effects of using a model trained on a different domain than at inference time. When using an Synthetic FOREST image degraded with the baseline degradation model as an input, the model trained using real FOREST-2 data as the target domain generates several artifacts and underperforms severely in terms of PSNR. \relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.18}{\ignorespaces Effects of using a model trained with on different domain than at inference time. (a) shows the log magnitude of the radial average of the FFT for the SR images using different algorithms. (b) shows the amplification with respect to bicubic interpolation. \relax }}{70}{figure.caption.60}\protected@file@percent }
\newlabel{fig:5-target-prediction-with-domain-gap-fft}{{8.18}{70}{Effects of using a model trained with on different domain than at inference time. (a) shows the log magnitude of the radial average of the FFT for the SR images using different algorithms. (b) shows the amplification with respect to bicubic interpolation. \relax }{figure.caption.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.19}{\ignorespaces Effects of using a model trained with on different domain than at inference time, statistics over the whole validation dataset. (a) shows the log magnitude of the radial average of the FFT for the SR images using different algorithms. (b) shows the amplification with respect to bicubic interpolation. Painted areas represent ±1 standard deviations. \relax }}{71}{figure.caption.61}\protected@file@percent }
\newlabel{fig:5-target-amplification-statistics-with-domain-gap}{{8.19}{71}{Effects of using a model trained with on different domain than at inference time, statistics over the whole validation dataset. (a) shows the log magnitude of the radial average of the FFT for the SR images using different algorithms. (b) shows the amplification with respect to bicubic interpolation. Painted areas represent ±1 standard deviations. \relax }{figure.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.20}{\ignorespaces Performance obtained by super resolving the degraded synthetic FOREST images using different super resolution models. The Pearson correlation coefficient is represented by $\rho $.\relax }}{71}{figure.caption.62}\protected@file@percent }
\newlabel{fig:5-target-prediction-with-domain-gap-dataset}{{8.20}{71}{Performance obtained by super resolving the degraded synthetic FOREST images using different super resolution models. The Pearson correlation coefficient is represented by $\rho $.\relax }{figure.caption.62}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Domain gap assessment using non-referenced image quality assessment}{72}{subsection.8.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.21}{\ignorespaces Image quality assessment metrics for the different SR models using different datasets as input. \relax }}{72}{figure.caption.63}\protected@file@percent }
\newlabel{fig:5-target-iqa-results}{{8.21}{72}{Image quality assessment metrics for the different SR models using different datasets as input. \relax }{figure.caption.63}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusions}{73}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Future Work}{74}{subsection.9.1}\protected@file@percent }
\citation{myself2023}
\bibstyle{unsrt}
\bibdata{bibliography}
\bibcite{lst2005}{1}
\bibcite{townshend94}{2}
\bibcite{author2023thermal}{3}
\bibcite{sca2009}{4}
\bibcite{mwa2001}{5}
\bibcite{LI201314}{6}
\bibcite{becker90}{7}
\bibcite{HORNING20082986}{8}
\bibcite{UNEP2021Wildfire}{9}
\bibcite{lippitt2015time}{10}
\bibcite{ijgi11120601}{11}
\bibcite{atlanticcouncil2021extreme}{12}
\bibcite{Hsu2021Disproportionate}{13}
\bibcite{deilami2018urban}{14}
\bibcite{mohamed2017land}{15}
\bibcite{sobrino2012impact}{16}
\bibcite{huang2013generating}{17}
\bibcite{USGS2023Landsat}{18}
\bibcite{terra_nasa}{19}
\bibcite{Zhu2021}{20}
\bibcite{Shi2019}{21}
\bibcite{Yang2017}{22}
\bibcite{Rouse1973MonitoringVS}{23}
\bibcite{skopje2018}{24}
\bibcite{rs11050573}{25}
\bibcite{rs13081524}{26}
\bibcite{rs12182949}{27}
\bibcite{zeyde2010single}{28}
\bibcite{martin2001database}{29}
\bibcite{valsesia2021permutation}{30}
\bibcite{bashir2021comprehensive}{31}
\bibcite{Liu2019}{32}
\bibcite{timofte2015seven}{33}
\bibcite{lai2017deep}{34}
\bibcite{Dai2019}{35}
\bibcite{zhang2018image}{36}
\bibcite{simonyan2015deep}{37}
\bibcite{he2015deep}{38}
\bibcite{ledig2017photorealistic}{39}
\bibcite{wang2018recovering}{40}
\bibcite{goodfellow2014generative}{41}
\bibcite{mao2017squares}{42}
\bibcite{martens2019superresolution}{43}
\bibcite{Salvetti_2020}{44}
\bibcite{Bordone_Molini_2020}{45}
\bibcite{MISR2007}{46}
\bibcite{myself2023}{47}
\bibcite{lugmayr2020ntire}{48}
\bibcite{liu2021blind}{49}
\bibcite{accurateblurs2013}{50}
\bibcite{zhang2018residual}{51}
\bibcite{gu2019blind}{52}
\bibcite{luo2020unfolding}{53}
\bibcite{zhou19}{54}
\bibcite{zontak2011}{55}
\bibcite{glasner2009}{56}
\bibcite{bellkligler2020blind}{57}
\bibcite{shocher2017zeroshot}{58}
\bibcite{CycleGAN2017}{59}
\bibcite{yuan2018unsupervised}{60}
\bibcite{luo2022learning}{61}
\bibcite{bulat2018learn}{62}
\bibcite{wei2020unsupervised}{63}
\bibcite{zhu2020unpaired}{64}
\bibcite{plotz2017benchmarking}{65}
\bibcite{isola2018imagetoimage}{66}
\bibcite{fritsche2019frequency}{67}
\bibcite{ECOSTRESS2023INSTRUMENT}{68}
\bibcite{VGGnet}{69}
\bibcite{zhang2018unreasonable}{70}
\bibcite{niqe}{71}
\bibcite{mittal2012}{72}
\bibcite{fuoli2021fourier}{73}
\bibcite{Sobel1990AnI3}{74}
\bibcite{ECOSTRESS2023}{75}
\bibcite{PhyTIR2023}{76}
\bibcite{AppEEARS2023}{77}
\bibcite{AppEEARSAPI2023}{78}
\bibcite{ECO1BMAPRAD2023}{79}
\bibcite{ecostress_faq}{80}
\gdef \@abspage@last{85}
