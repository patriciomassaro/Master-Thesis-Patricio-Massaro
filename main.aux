\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\citation{lst2005}
\citation{townshend94}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Thermal Remote Sensing}{2}{section.2}\protected@file@percent }
\newlabel{sec:thermal_remote_sensing}{{2}{2}{Thermal Remote Sensing}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Electromagnetic spectrum}{2}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Electromagnetic spectrum\relax }}{3}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1-electromagnetic-spectrum}{{2.1}{3}{Electromagnetic spectrum\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Land surface temperature}{3}{subsection.2.2}\protected@file@percent }
\citation{becker90}
\citation{sca2009}
\citation{mwa2001}
\citation{LI201314}
\citation{HORNING20082986}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Quality dimensions of remote sensing data}{4}{subsection.2.3}\protected@file@percent }
\citation{UNEP2021Wildfire}
\@writefile{toc}{\contentsline {section}{\numberline {3}Motivation}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Wildfire Monitoring}{5}{subsection.3.1}\protected@file@percent }
\citation{lippitt2015time}
\citation{ijgi11120601}
\citation{atlanticcouncil2021extreme}
\citation{Hsu2021Disproportionate}
\citation{deilami2018urban}
\citation{mohamed2017land}
\citation{sobrino2012impact}
\citation{huang2013generating}
\citation{USGS2023Landsat}
\citation{terra_nasa}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Urban heat}{6}{subsection.3.2}\protected@file@percent }
\citation{Zhu2021}
\citation{Shi2019}
\citation{Shi2019}
\citation{Yang2017}
\citation{deilami2018urban}
\citation{Rouse1973MonitoringVS}
\citation{skopje2018}
\citation{Rouse1973MonitoringVS}
\citation{skopje2018}
\newlabel{fig:1-skopje-NDVI}{{\caption@xref {fig:1-skopje-NDVI}{ on input line 189}}{7}{Urban heat}{figure.caption.4}{}}
\newlabel{fig:1-skopje-LST}{{\caption@xref {fig:1-skopje-LST}{ on input line 194}}{7}{Urban heat}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Normalized Difference Vegetation Index \cite  {Rouse1973MonitoringVS} and LST measurements for the zone of Skopje, North Macedonia. Urban areas with a lower vegetation index tend to have a higher temperature than their rural counterparts. Source : \cite  {skopje2018}.\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:1-skopie-UHI}{{3.1}{7}{Normalized Difference Vegetation Index \cite {Rouse1973MonitoringVS} and LST measurements for the zone of Skopje, North Macedonia. Urban areas with a lower vegetation index tend to have a higher temperature than their rural counterparts. Source : \cite {skopje2018}.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}The spatio-temporal trade off}{7}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Scatter plot of the spatial and temporal resolution of some of the LST/TIR data products available. The trade-off is evident, no mission can provide products in the zone of interest. Constellations may help with temporal resolution, but the spatial resolution is still limited.\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:1-spatio-temporal-trade-off}{{3.2}{8}{Scatter plot of the spatial and temporal resolution of some of the LST/TIR data products available. The trade-off is evident, no mission can provide products in the zone of interest. Constellations may help with temporal resolution, but the spatial resolution is still limited.\relax }{figure.caption.5}{}}
\citation{zeyde2010single}
\citation{martin2001database}
\citation{valsesia2021permutation}
\citation{bashir2021comprehensive}
\@writefile{toc}{\contentsline {section}{\numberline {4}Super resolution}{9}{section.4}\protected@file@percent }
\newlabel{sec:SR}{{4}{9}{Super resolution}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Example of super resolution as an ill posed problem. A blurry picture of Barack Obama can be generated from an HR image of another person.\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:2-SR-ill-posed}{{4.1}{9}{Example of super resolution as an ill posed problem. A blurry picture of Barack Obama can be generated from an HR image of another person.\relax }{figure.caption.6}{}}
\citation{bashir2021comprehensive}
\citation{bashir2021comprehensive}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces In traditional machine learning, the feature extraction step is crucial for performance, requiring a lot of domain knowledge. In deep learning, the feature extraction is learned from the data.\relax }}{10}{figure.caption.7}\protected@file@percent }
\newlabel{fig:2-end-to-end-training}{{4.2}{10}{In traditional machine learning, the feature extraction step is crucial for performance, requiring a lot of domain knowledge. In deep learning, the feature extraction is learned from the data.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Example of generating a super resolution dataset, using a simplified known degradation model. Source: \cite  {bashir2021comprehensive}.\relax }}{11}{figure.caption.8}\protected@file@percent }
\newlabel{fig:3-super-resolution-data}{{4.3}{11}{Example of generating a super resolution dataset, using a simplified known degradation model. Source: \cite {bashir2021comprehensive}.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Single-Image Super Resolution}{11}{subsection.4.1}\protected@file@percent }
\newlabel{eq:2-degradation-equation}{{2}{11}{Single-Image Super Resolution}{equation.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Upsampling method}{11}{subsubsection.4.1.1}\protected@file@percent }
\citation{timofte2015seven}
\citation{lai2017deep}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Network design}{12}{subsubsection.4.1.2}\protected@file@percent }
\newlabel{eq:2-residual-learning}{{3}{12}{Network design}{equation.4.3}{}}
\citation{simonyan2015deep}
\citation{he2015deep}
\citation{ledig2017photorealistic}
\citation{wang2018recovering}
\citation{goodfellow2014generative}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Loss functions}{13}{subsubsection.4.1.3}\protected@file@percent }
\citation{ledig2017photorealistic}
\citation{ledig2017photorealistic}
\citation{martens2019superresolution}
\citation{Salvetti_2020}
\citation{Bordone_Molini_2020}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Illustration of patches from the natural image manifold and results coming from MSE pixel-loss (red) and GANs (orange). source:\cite  {ledig2017photorealistic}\relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig:2-gans-natural-manifold}{{4.4}{14}{Illustration of patches from the natural image manifold and results coming from MSE pixel-loss (red) and GANs (orange). source:\cite {ledig2017photorealistic}\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Multi-Image Super Resolution}{14}{subsection.4.2}\protected@file@percent }
\citation{MISR2007}
\citation{MISR2007}
\citation{myself2023}
\citation{myself2023}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Multi-image super resolution algorithms combine multiple low-resolution image acquisitions into a high-resolution image. Source: \cite  {MISR2007}\relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{fig:2-MISR}{{4.5}{15}{Multi-image super resolution algorithms combine multiple low-resolution image acquisitions into a high-resolution image. Source: \cite {MISR2007}\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Multi-spectral super resolution}{15}{subsubsection.4.2.1}\protected@file@percent }
\citation{myself2023}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Example of Pan-sharpening on TIR data using a panchromatic band. (a) Panchromatic band, (b) HR TIR image, (c) Downsampled version of the TIR image , (d) Pansharpened image. The pan-sharpened image is less blurry than the LR, but a lot of artifacts are produced, specially in clouds. Source: \cite  {myself2023}\relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{fig:2-pansharpening}{{4.6}{16}{Example of Pan-sharpening on TIR data using a panchromatic band. (a) Panchromatic band, (b) HR TIR image, (c) Downsampled version of the TIR image , (d) Pansharpened image. The pan-sharpened image is less blurry than the LR, but a lot of artifacts are produced, specially in clouds. Source: \cite {myself2023}\relax }{figure.caption.11}{}}
\citation{lugmayr2020ntire}
\citation{liu2021blind}
\citation{liu2021blind}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}The domain gap problem}{17}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:domaingap}{{4.3}{17}{The domain gap problem}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Effects of different degradation models on one HR image. Source: \cite  {liu2021blind}\relax }}{17}{figure.caption.12}\protected@file@percent }
\newlabel{fig:2-domain-gap}{{4.7}{17}{Effects of different degradation models on one HR image. Source: \cite {liu2021blind}\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Blind image Super Resolution}{17}{subsection.4.4}\protected@file@percent }
\citation{liu2021blind}
\citation{liu2021blind}
\citation{zhang2018residual}
\citation{zhang2018residual}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Domain interpretation of differences between non-blind and blind SR. Source: \cite  {liu2021blind}\relax }}{18}{figure.caption.13}\protected@file@percent }
\newlabel{fig:2-DomainGap}{{4.8}{18}{Domain interpretation of differences between non-blind and blind SR. Source: \cite {liu2021blind}\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Explicit modelling with external dataset}{18}{subsubsection.4.4.1}\protected@file@percent }
\citation{gu2019blind}
\citation{luo2020unfolding}
\citation{zontak2011}
\citation{glasner2009}
\citation{bellkligler2020blind}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Dimensionality stretching strategy to concatenate the degradation map to the LR input. The vectorized kernel is projected onto a space of a lower dimensionality, and then stretched to generate $t$ feature maps with the same shape of the input image. The noise level is also concatenated. Source: \cite  {zhang2018residual} \relax }}{19}{figure.caption.14}\protected@file@percent }
\newlabel{fig:2-external-dataset-stretching}{{4.9}{19}{Dimensionality stretching strategy to concatenate the degradation map to the LR input. The vectorized kernel is projected onto a space of a lower dimensionality, and then stretched to generate $t$ feature maps with the same shape of the input image. The noise level is also concatenated. Source: \cite {zhang2018residual} \relax }{figure.caption.14}{}}
\citation{bellkligler2020blind}
\citation{bellkligler2020blind}
\citation{shocher2017zeroshot}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Explicit modelling with single image}{20}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces KernelGAN schematic diagram. The discriminator tries to distinguish between the generated patches and the original LR image patches. G learns perform 2x downscaling while fooling the discriminator by maintaining the same distribution of patches. Source \cite  {bellkligler2020blind}\relax }}{20}{figure.caption.15}\protected@file@percent }
\newlabel{fig:2-kernelGAN}{{4.10}{20}{KernelGAN schematic diagram. The discriminator tries to distinguish between the generated patches and the original LR image patches. G learns perform 2x downscaling while fooling the discriminator by maintaining the same distribution of patches. Source \cite {bellkligler2020blind}\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Implicit modelling}{20}{subsubsection.4.4.3}\protected@file@percent }
\newlabel{subsubsec:implicit-modelling}{{4.4.3}{20}{Implicit modelling}{subsubsection.4.4.3}{}}
\citation{CycleGAN2017}
\citation{yuan2018unsupervised}
\citation{luo2022learning}
\citation{bulat2018learn}
\citation{wei2020unsupervised}
\citation{bulat2018learn}
\citation{bulat2018learn}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Degradation GAN schematic diagram. The architecture includes one LR generator, on SR network and two discriminators. Source: \cite  {bulat2018learn}.\relax }}{22}{figure.caption.16}\protected@file@percent }
\newlabel{fig:2-degradation-gan}{{4.11}{22}{Degradation GAN schematic diagram. The architecture includes one LR generator, on SR network and two discriminators. Source: \cite {bulat2018learn}.\relax }{figure.caption.16}{}}
\citation{luo2022learning}
\citation{zhu2020unpaired}
\@writefile{toc}{\contentsline {section}{\numberline {5}Methodology}{23}{section.5}\protected@file@percent }
\newlabel{sec:methodology}{{5}{23}{Methodology}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Models Architecture}{23}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Probabilistic degradation model}{23}{subsubsection.5.1.1}\protected@file@percent }
\citation{plotz2017benchmarking}
\citation{bulat2018learn}
\citation{wei2020unsupervised}
\citation{bulat2018learn}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Schematic of the probabilistic degradation module. The discriminator is left out for a more intuitive description.\relax }}{24}{figure.caption.17}\protected@file@percent }
\newlabel{fig:3-probabilistic-degradation-model}{{5.1}{24}{Schematic of the probabilistic degradation module. The discriminator is left out for a more intuitive description.\relax }{figure.caption.17}{}}
\citation{isola2018imagetoimage}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Schematic of the generative networks used in the kernel and noise module of the probabilistic degradation model. The parameters of the convolutional layers represent input channels, output channels, kernel size, stride and padding, respectively. The residual blocks use the same kernel size as the convolutional layers of each module. In the noise module, the random vector $z_n$ is concatenated with $I_{\text  {clean}}^{\text  {LR}}$ before the first convolutional layer. \relax }}{25}{figure.caption.18}\protected@file@percent }
\newlabel{fig:3-slim-gen-module}{{5.2}{25}{Schematic of the generative networks used in the kernel and noise module of the probabilistic degradation model. The parameters of the convolutional layers represent input channels, output channels, kernel size, stride and padding, respectively. The residual blocks use the same kernel size as the convolutional layers of each module. In the noise module, the random vector $z_n$ is concatenated with $I_{\text {clean}}^{\text {LR}}$ before the first convolutional layer. \relax }{figure.caption.18}{}}
\citation{fritsche2019frequency}
\citation{wei2020unsupervised}
\citation{luo2022learning}
\citation{luo2022learning}
\citation{myself2023}
\citation{ledig2017photorealistic}
\citation{simonyan2015deep}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Diagram of the PatchGAN discriminator. The parameters of the convolutional layers represent input channels, output channels, kernel size, stride and padding, respectively. \relax }}{26}{figure.caption.19}\protected@file@percent }
\newlabel{fig:3-slim-patchgan-module}{{5.3}{26}{Diagram of the PatchGAN discriminator. The parameters of the convolutional layers represent input channels, output channels, kernel size, stride and padding, respectively. \relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The probabilistic degradation model is used to encourage the degradation model to produce images in the same domain as the test LR images. After training, the SR model is directly used to super resolve the inputs. Source \cite  {luo2022learning}.\relax }}{26}{figure.caption.20}\protected@file@percent }
\newlabel{fig:3-GAN-degradation-model}{{5.4}{26}{The probabilistic degradation model is used to encourage the degradation model to produce images in the same domain as the test LR images. After training, the SR model is directly used to super resolve the inputs. Source \cite {luo2022learning}.\relax }{figure.caption.20}{}}
\citation{zhang2018residual}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}SRResNet}{27}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces  Modified SRResNet architecture. $X_{LR}$ represents the low resolution input image, $X_{SR}$ the super resolved image, which is then compared to the ground truth $X_{HR}$.\relax }}{27}{figure.caption.21}\protected@file@percent }
\newlabel{fig:3-resnet-architecture}{{5.5}{27}{Modified SRResNet architecture. $X_{LR}$ represents the low resolution input image, $X_{SR}$ the super resolved image, which is then compared to the ground truth $X_{HR}$.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Baseline Degradation model}{27}{subsection.5.2}\protected@file@percent }
\newlabel{subsec:baseline_degradation_model}{{5.2}{27}{Baseline Degradation model}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Blurring Kernel}{28}{subsubsection.5.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Example of kernels used in a stochastic degradation model. (a),(b) and (c) are generated using a symmetric variance on the x and y axis. (d) (e) and (f) are generated using an asymmetric variances, resulting in much more anisotropic kernels.\relax }}{28}{figure.caption.22}\protected@file@percent }
\newlabel{fig:4-degradation_kernels}{{5.6}{28}{Example of kernels used in a stochastic degradation model. (a),(b) and (c) are generated using a symmetric variance on the x and y axis. (d) (e) and (f) are generated using an asymmetric variances, resulting in much more anisotropic kernels.\relax }{figure.caption.22}{}}
\citation{ECOSTRESS2023INSTRUMENT}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Effects of different blurring kernels on the HR-LR generation. The upper row contains images generated using blurring kernels with symmetric distributions. The lower rows contains images generated using asymmetric distributions for the variances, resulting in highly anisotropic kernels.\relax }}{29}{figure.caption.23}\protected@file@percent }
\newlabel{fig:4-degradation-kernel-examples}{{5.7}{29}{Effects of different blurring kernels on the HR-LR generation. The upper row contains images generated using blurring kernels with symmetric distributions. The lower rows contains images generated using asymmetric distributions for the variances, resulting in highly anisotropic kernels.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Radiometric error correction}{29}{subsubsection.5.2.2}\protected@file@percent }
\newlabel{eq:4-radiometric-error-correction}{{15}{29}{Radiometric error correction}{equation.5.15}{}}
\newlabel{eq:4-planck-derivative}{{16}{29}{Radiometric error correction}{equation.5.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Effects of increasing radiometric error on the HR-LR generation.\relax }}{30}{figure.caption.24}\protected@file@percent }
\newlabel{fig:4-radiometric_noise_example}{{5.8}{30}{Effects of increasing radiometric error on the HR-LR generation.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Signal-to-Noise Ratio (SNR)}{31}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Referenced image quality metrics}{31}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}pixel-wise losses}{31}{subsubsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}Peak Signal-to-Noise Ratio (PSNR)}{31}{subsubsection.5.4.2}\protected@file@percent }
\citation{VGGnet}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3}Structural Similarity Index (SSIM)}{32}{subsubsection.5.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.4}Learned perceptual image patch similarity (LPIPS)}{32}{subsubsection.5.4.4}\protected@file@percent }
\citation{martens2019superresolution}
\citation{niqe}
\citation{niqe}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.5}Adjusting measures to a slight translations in the SR process.}{33}{subsubsection.5.4.5}\protected@file@percent }
\newlabel{eq:4_adjusted_metrics}{{22}{33}{Adjusting measures to a slight translations in the SR process}{equation.5.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Non-referenced Image quality metrics}{33}{subsection.5.5}\protected@file@percent }
\citation{mittal2012}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Workflow of a NR-IQA model.\relax }}{34}{figure.caption.25}\protected@file@percent }
\newlabel{fig:4-nr-iqa-workflow}{{5.9}{34}{Workflow of a NR-IQA model.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.1}Naturalness Image Quality Evaluator (NIQE)}{34}{subsubsection.5.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2}Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE)}{34}{subsubsection.5.5.2}\protected@file@percent }
\citation{fuoli2021fourier}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.3}Frequency Domain Analysis}{35}{subsubsection.5.5.3}\protected@file@percent }
\newlabel{subsubsec:frequency_domain_analysis}{{5.5.3}{35}{Frequency Domain Analysis}{subsubsection.5.5.3}{}}
\citation{Sobel1990AnI3}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Steps of the frequency domain analysis. The Center image shows the log magnitude of the shifted FFT of a bicubic upsampled FOREST scene and an example of a radial profile, the average of all the points that have the same $r$ is calculated. The right image displays the log magnitude obtained for every radial profile, translated into spatial frequency. \relax }}{36}{figure.caption.26}\protected@file@percent }
\newlabel{fig:4-frequency-analysis}{{5.10}{36}{Steps of the frequency domain analysis. The Center image shows the log magnitude of the shifted FFT of a bicubic upsampled FOREST scene and an example of a radial profile, the average of all the points that have the same $r$ is calculated. The right image displays the log magnitude obtained for every radial profile, translated into spatial frequency. \relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.4}Gradient Distribution analysis}{36}{subsubsection.5.5.4}\protected@file@percent }
\newlabel{eq:4-sobel-operators}{{25}{36}{Gradient Distribution analysis}{equation.5.25}{}}
\newlabel{eq:4-gradient_magnitude}{{26}{36}{Gradient Distribution analysis}{equation.5.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Steps to obtain a gradient magnitude density. Using the sobel operators, $G_x$ and $G_y$ are obtained from an image. The magnitude $|G|$ of each pixel is calculated using Eq. \ref {eq:4-gradient_magnitude}. The density can be estimated afterwards, using 100 bins in this case.\relax }}{37}{figure.caption.27}\protected@file@percent }
\newlabel{fig:4-gradient-analysis}{{5.11}{37}{Steps to obtain a gradient magnitude density. Using the sobel operators, $G_x$ and $G_y$ are obtained from an image. The magnitude $|G|$ of each pixel is calculated using Eq. \ref {eq:4-gradient_magnitude}. The density can be estimated afterwards, using 100 bins in this case.\relax }{figure.caption.27}{}}
\citation{ECOSTRESS2023}
\citation{PhyTIR2023}
\citation{ECOSTRESS2023}
\@writefile{toc}{\contentsline {section}{\numberline {6}Datasets}{38}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Obtaining a high resolution dataset}{38}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}The ECOSTRESS mission}{38}{subsubsection.6.1.1}\protected@file@percent }
\citation{AppEEARS2023}
\citation{AppEEARSAPI2023}
\citation{ECO1BMAPRAD2023}
\citation{ecostress_faq}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Wavelengths of the sensors in Ecostress and Forest satellites. The radiation spectrum of black-bodies at different temperatures are included for comparison.\relax }}{39}{figure.caption.28}\protected@file@percent }
\newlabel{fig:5-wavelength-comparison}{{6.1}{39}{Wavelengths of the sensors in Ecostress and Forest satellites. The radiation spectrum of black-bodies at different temperatures are included for comparison.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Accessing ECOSTRESS Scenes}{39}{subsubsection.6.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Requests configuration\relax }}{39}{table.caption.29}\protected@file@percent }
\newlabel{tab:5-scenes-characteristics}{{6.1}{39}{Requests configuration\relax }{table.caption.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Selecting the best scenes}{39}{subsubsection.6.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Location of the samples taken from ecostress.\relax }}{40}{figure.caption.30}\protected@file@percent }
\newlabel{fig:5-ecostress-map-location}{{6.2}{40}{Location of the samples taken from ecostress.\relax }{figure.caption.30}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Process applied to the scenes returned from one area request.\relax }}{40}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.4}Data Processing}{41}{subsubsection.6.1.4}\protected@file@percent }
\newlabel{tab:quality_classes}{{\caption@xref {tab:quality_classes}{ on input line 113}}{41}{Data Processing}{table.caption.31}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Fill Value and Data Quality Classes\relax }}{41}{table.caption.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Data processing workflow\relax }}{42}{figure.caption.32}\protected@file@percent }
\newlabel{fig:5-data_processing_flow_chart}{{6.3}{42}{Data processing workflow\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Random crop processor\relax }}{42}{figure.caption.33}\protected@file@percent }
\newlabel{fig:5-random_crop_processor}{{6.4}{42}{Random crop processor\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Obtaining FOREST-2 data}{42}{subsection.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Location of the FOREST-2 scenes.\relax }}{43}{figure.caption.34}\protected@file@percent }
\newlabel{fig:4-forest-locations}{{6.5}{43}{Location of the FOREST-2 scenes.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces LWIR1, LWIR2 and MWIR bands of a FOREST-2 scene downloaded from the company's API.\relax }}{43}{figure.caption.35}\protected@file@percent }
\newlabel{fig:4-forest-complete example}{{6.6}{43}{LWIR1, LWIR2 and MWIR bands of a FOREST-2 scene downloaded from the company's API.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces LWIR1 and LWIR2 of a FOREST-2 scene downloaded from the company's API, after cropping NA values.\relax }}{44}{figure.caption.36}\protected@file@percent }
\newlabel{fig:4-forest-bounding-box}{{6.7}{44}{LWIR1 and LWIR2 of a FOREST-2 scene downloaded from the company's API, after cropping NA values.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Datasets}{44}{subsection.6.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces \relax }}{44}{table.caption.37}\protected@file@percent }
\newlabel{tab:my-table}{{6.3}{44}{\relax }{table.caption.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Synthetic FOREST - Degraded Synthetic FOREST}{45}{subsubsection.6.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Parameters used in the degradation model employed to generate the $\mathcal  {D}_{\text  {SF}-\text  {SF}}$ dataset.\relax }}{45}{table.caption.38}\protected@file@percent }
\newlabel{tab:degradation_model_parameters}{{6.4}{45}{Parameters used in the degradation model employed to generate the $\mathcal {D}_{\text {SF}-\text {SF}}$ dataset.\relax }{table.caption.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Synthetic FOREST - real FOREST (Unpaired)}{45}{subsubsection.6.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.3}Synthetic FOREST- real FOREST ( Paired)}{45}{subsubsection.6.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Experiment Setup}{46}{section.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Experiment setup parameters\relax }}{47}{table.caption.39}\protected@file@percent }
\newlabel{tab:experiment-setup}{{7.1}{47}{Experiment setup parameters\relax }{table.caption.39}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Results and discussion}{48}{section.8}\protected@file@percent }
\newlabel{sec:results}{{8}{48}{Results and discussion}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Source domain}{48}{subsection.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Applying different degradation models on an HR sample. The 2 most upper rows show the estimated degradation kernels and noise of each pipeline, the bicubic downsampling does not estimate a kernel or noise. The degraded LR images from each model and a zoom is displayed on the two subsequent rows. In this case, the PSNR is calculated against the gaussian blurring + bicubic downsampling LR. The synthetic FOREST-2 (ground truth) and the super resolved images, with a zoom, are displayed in the last 2 rows. The PSNR for each SR method is calculated against the HR synthetic FOREST-2. \relax }}{50}{figure.caption.40}\protected@file@percent }
\newlabel{fig:5-source_domain_sample}{{8.1}{50}{Applying different degradation models on an HR sample. The 2 most upper rows show the estimated degradation kernels and noise of each pipeline, the bicubic downsampling does not estimate a kernel or noise. The degraded LR images from each model and a zoom is displayed on the two subsequent rows. In this case, the PSNR is calculated against the gaussian blurring + bicubic downsampling LR. The synthetic FOREST-2 (ground truth) and the super resolved images, with a zoom, are displayed in the last 2 rows. The PSNR for each SR method is calculated against the HR synthetic FOREST-2. \relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Log mangnitude of the FFT for the LR images obtained by the pipelines and the gaussian blurring + bicubic upsampling.\relax }}{51}{figure.caption.41}\protected@file@percent }
\newlabel{fig:5-lr-images-fft.pdf}{{8.2}{51}{Log mangnitude of the FFT for the LR images obtained by the pipelines and the gaussian blurring + bicubic upsampling.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces (a) Radial profile of the log magnitude across spatial frequency of the LR images obtained by the pipelines and the gaussian blurring + bicubic downsampling model. (b) Amplification in dB of each pipeline with respect to the gaussian blurring + bicubic downsampling.\relax }}{52}{figure.caption.42}\protected@file@percent }
\newlabel{fig:5-lr-images-fft-comparison.pdf}{{8.3}{52}{(a) Radial profile of the log magnitude across spatial frequency of the LR images obtained by the pipelines and the gaussian blurring + bicubic downsampling model. (b) Amplification in dB of each pipeline with respect to the gaussian blurring + bicubic downsampling.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Frequency domain analysis of the SR images and the ground truth displayed in \ref {fig:5-source_domain_sample}. In (a), the log of the magnitude of the FFT for the SR images and the ground truth is shown, while in (b), the amplification of each SR image with respect to the ground truth is shown.\relax }}{53}{figure.caption.43}\protected@file@percent }
\newlabel{fig:5-source-sr-fft-comparison}{{8.4}{53}{Frequency domain analysis of the SR images and the ground truth displayed in \ref {fig:5-source_domain_sample}. In (a), the log of the magnitude of the FFT for the SR images and the ground truth is shown, while in (b), the amplification of each SR image with respect to the ground truth is shown.\relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}Probabilistic degradation models comparison}{53}{subsubsection.8.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Mean and standard deviation of the estimated kernels for the baseline and adapted degradation model, using 2000 realizations of $z_k$. The standard deviation of each pixel is normalized by the corresponding mean value. kernel pixels with mean lower than $10^{-4}$ are considered with 0 std for clarity in the plot.\relax }}{54}{figure.caption.44}\protected@file@percent }
\newlabel{fig:5-source-kernel-mean-std}{{8.5}{54}{Mean and standard deviation of the estimated kernels for the baseline and adapted degradation model, using 2000 realizations of $z_k$. The standard deviation of each pixel is normalized by the corresponding mean value. kernel pixels with mean lower than $10^{-4}$ are considered with 0 std for clarity in the plot.\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Distribution of SNR values using $I_{\text  {LR}}^{\text  {clean}}$, product of the convolution of the kernel and $I_{\text  {HR}}^{\text  {clean}}$, and the noise module output for both pipelines. The output noise is generated 2000 times, using different realizations of the random variable $z_n$ for each iteration and the same input image. \relax }}{55}{figure.caption.45}\protected@file@percent }
\newlabel{fig:5-source-noise-1-sample}{{8.6}{55}{Distribution of SNR values using $I_{\text {LR}}^{\text {clean}}$, product of the convolution of the kernel and $I_{\text {HR}}^{\text {clean}}$, and the noise module output for both pipelines. The output noise is generated 2000 times, using different realizations of the random variable $z_n$ for each iteration and the same input image. \relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces Comparison of the SNR expressed in dB of the low resolution images generated by the baseline and adapted degradation model.\relax }}{56}{figure.caption.46}\protected@file@percent }
\newlabel{fig:5-source-noise-SNR}{{8.7}{56}{Comparison of the SNR expressed in dB of the low resolution images generated by the baseline and adapted degradation model.\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Low resolution images comparison}{56}{subsubsection.8.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces Performance metrics between the LR images obtained by the pipelines vs the gaussian blurring + bicubic downsampling degradation. On the left, the PSNR is displayed. On the middle and the right, SSIM and LPIPS are represented respectively.\relax }}{57}{figure.caption.47}\protected@file@percent }
\newlabel{fig:5-source-domain-lr-performance-scatterplot}{{8.8}{57}{Performance metrics between the LR images obtained by the pipelines vs the gaussian blurring + bicubic downsampling degradation. On the left, the PSNR is displayed. On the middle and the right, SSIM and LPIPS are represented respectively.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces Frequency domain analysis of the LR images obtained by applying different degradation models on the HR sample displayed in Fig. \ref {fig:5-source_domain_sample}. In (a), the log of the magnitude of the FFT for the LR images is shown, while in (b), the amplification with respect to a simple gaussian blurring + downscaling is shown. The painted area represents the ±1 standard deviation of the radial profiles and the amplification. \relax }}{58}{figure.caption.48}\protected@file@percent }
\newlabel{fig:5-lr-images-fft-comparison}{{8.9}{58}{Frequency domain analysis of the LR images obtained by applying different degradation models on the HR sample displayed in Fig. \ref {fig:5-source_domain_sample}. In (a), the log of the magnitude of the FFT for the LR images is shown, while in (b), the amplification with respect to a simple gaussian blurring + downscaling is shown. The painted area represents the ±1 standard deviation of the radial profiles and the amplification. \relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.3}Effects of the degradation model in SR}{58}{subsubsection.8.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces Performance obtained by super resolving the degraded images coming out of the generator. In (a), the corresponding SR model of each pipeline is used. In (b), a simple bicubic upsampling is used to super resolve the degraded images. \relax }}{59}{figure.caption.49}\protected@file@percent }
\newlabel{fig:5-source-domain-comparison}{{8.10}{59}{Performance obtained by super resolving the degraded images coming out of the generator. In (a), the corresponding SR model of each pipeline is used. In (b), a simple bicubic upsampling is used to super resolve the degraded images. \relax }{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Target domain}{59}{subsection.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces Super Resolved Forest-2 Scene using different SR models. In the upper row, the image is displayed. A detailed zoom is displayed below. The bottom row shows the log magnitude of the FFT for the images. The original image is displayed in the left, while the super resolved images are displayed afterwards. \relax }}{60}{figure.caption.50}\protected@file@percent }
\newlabel{fig:5-target_prediction_sample}{{8.11}{60}{Super Resolved Forest-2 Scene using different SR models. In the upper row, the image is displayed. A detailed zoom is displayed below. The bottom row shows the log magnitude of the FFT for the images. The original image is displayed in the left, while the super resolved images are displayed afterwards. \relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces Frequency domain analysis of the SR images obtained by applying different SR models to the real FOREST-2 validation dataset. In (a), the log of the magnitude of the FFT for the SR images is shown, while in (b), the amplification with respect to a simple bicubic upsampling is displayed.\relax }}{61}{figure.caption.51}\protected@file@percent }
\newlabel{fig:5-target-amplification-statistics}{{8.12}{61}{Frequency domain analysis of the SR images obtained by applying different SR models to the real FOREST-2 validation dataset. In (a), the log of the magnitude of the FFT for the SR images is shown, while in (b), the amplification with respect to a simple bicubic upsampling is displayed.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces Gradient analysis of the super resolved images using different SR models for scenes coming from the real FOREST-2 validation dataset. In the upper row, the image is displayed. The gradients in the x and y direction ($G_x$ and $G_y$ respectiely) are displayed below. the gradient magnitude $|G|$ is displayed in the bottom row.\relax }}{62}{figure.caption.52}\protected@file@percent }
\newlabel{fig:6-target-gradient-analysis-image}{{8.13}{62}{Gradient analysis of the super resolved images using different SR models for scenes coming from the real FOREST-2 validation dataset. In the upper row, the image is displayed. The gradients in the x and y direction ($G_x$ and $G_y$ respectiely) are displayed below. the gradient magnitude $|G|$ is displayed in the bottom row.\relax }{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.14}{\ignorespaces Histogram of the gradient magnitude $|G|$ for the whole validation real FOREST-2 dataset.\relax }}{63}{figure.caption.53}\protected@file@percent }
\newlabel{fig:5-gradient-histogram-validation-dataset}{{8.14}{63}{Histogram of the gradient magnitude $|G|$ for the whole validation real FOREST-2 dataset.\relax }{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}The domain gap goes both ways}{63}{subsection.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.15}{\ignorespaces Effects of using a model trained with on different domain than at inference time. When using an Synthetic FOREST image degraded with the baseline degradation model as an input, the model trained using real FOREST-2 data as the target domain generates several artifacts and underperforms severly in terms of PSNR. \relax }}{64}{figure.caption.54}\protected@file@percent }
\newlabel{fig:5-target-prediction-with-domain-gap}{{8.15}{64}{Effects of using a model trained with on different domain than at inference time. When using an Synthetic FOREST image degraded with the baseline degradation model as an input, the model trained using real FOREST-2 data as the target domain generates several artifacts and underperforms severly in terms of PSNR. \relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.16}{\ignorespaces Effects of using a model trained with on different domain than at inference time. (a) shows the log magnitude of the radial average of the FFT for the SR images using different algorithms. (b) shows the amplification with respect to bicubic interpolation. \relax }}{65}{figure.caption.55}\protected@file@percent }
\newlabel{fig:5-target-prediction-with-domain-gap-fft}{{8.16}{65}{Effects of using a model trained with on different domain than at inference time. (a) shows the log magnitude of the radial average of the FFT for the SR images using different algorithms. (b) shows the amplification with respect to bicubic interpolation. \relax }{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.17}{\ignorespaces Performance obtained by super resolving the degraded synthetic FOREST images using different super resolution models.\relax }}{65}{figure.caption.56}\protected@file@percent }
\newlabel{fig:5-target-prediction-with-domain-gap-dataset}{{8.17}{65}{Performance obtained by super resolving the degraded synthetic FOREST images using different super resolution models.\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Domain gap assessment using non-referenced image quality assessment}{66}{subsection.8.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.18}{\ignorespaces Image quality assessment metrics for the different SR models using different datasets as input. In both metrics, the lower the score, the better the image quality.\relax }}{66}{figure.caption.57}\protected@file@percent }
\newlabel{fig:5-target-iqa-results}{{8.18}{66}{Image quality assessment metrics for the different SR models using different datasets as input. In both metrics, the lower the score, the better the image quality.\relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusions and future work}{68}{section.9}\protected@file@percent }
\citation{myself2023}
\bibstyle{unsrt}
\bibdata{bibliography}
\bibcite{lst2005}{1}
\bibcite{townshend94}{2}
\bibcite{becker90}{3}
\bibcite{sca2009}{4}
\bibcite{mwa2001}{5}
\bibcite{LI201314}{6}
\bibcite{HORNING20082986}{7}
\bibcite{UNEP2021Wildfire}{8}
\bibcite{lippitt2015time}{9}
\bibcite{ijgi11120601}{10}
\bibcite{atlanticcouncil2021extreme}{11}
\bibcite{Hsu2021Disproportionate}{12}
\bibcite{deilami2018urban}{13}
\bibcite{mohamed2017land}{14}
\bibcite{sobrino2012impact}{15}
\bibcite{huang2013generating}{16}
\bibcite{USGS2023Landsat}{17}
\bibcite{terra_nasa}{18}
\bibcite{Zhu2021}{19}
\bibcite{Shi2019}{20}
\bibcite{Yang2017}{21}
\bibcite{Rouse1973MonitoringVS}{22}
\bibcite{skopje2018}{23}
\bibcite{zeyde2010single}{24}
\bibcite{martin2001database}{25}
\bibcite{valsesia2021permutation}{26}
\bibcite{bashir2021comprehensive}{27}
\bibcite{timofte2015seven}{28}
\bibcite{lai2017deep}{29}
\bibcite{simonyan2015deep}{30}
\bibcite{he2015deep}{31}
\bibcite{ledig2017photorealistic}{32}
\bibcite{wang2018recovering}{33}
\bibcite{goodfellow2014generative}{34}
\bibcite{martens2019superresolution}{35}
\bibcite{Salvetti_2020}{36}
\bibcite{Bordone_Molini_2020}{37}
\bibcite{MISR2007}{38}
\bibcite{myself2023}{39}
\bibcite{lugmayr2020ntire}{40}
\bibcite{liu2021blind}{41}
\bibcite{zhang2018residual}{42}
\bibcite{gu2019blind}{43}
\bibcite{luo2020unfolding}{44}
\bibcite{zontak2011}{45}
\bibcite{glasner2009}{46}
\bibcite{bellkligler2020blind}{47}
\bibcite{shocher2017zeroshot}{48}
\bibcite{CycleGAN2017}{49}
\bibcite{yuan2018unsupervised}{50}
\bibcite{luo2022learning}{51}
\bibcite{bulat2018learn}{52}
\bibcite{wei2020unsupervised}{53}
\bibcite{zhu2020unpaired}{54}
\bibcite{plotz2017benchmarking}{55}
\bibcite{isola2018imagetoimage}{56}
\bibcite{fritsche2019frequency}{57}
\bibcite{ECOSTRESS2023INSTRUMENT}{58}
\bibcite{VGGnet}{59}
\bibcite{niqe}{60}
\bibcite{mittal2012}{61}
\bibcite{fuoli2021fourier}{62}
\bibcite{Sobel1990AnI3}{63}
\bibcite{ECOSTRESS2023}{64}
\bibcite{PhyTIR2023}{65}
\bibcite{AppEEARS2023}{66}
\bibcite{AppEEARSAPI2023}{67}
\bibcite{ECO1BMAPRAD2023}{68}
\bibcite{ecostress_faq}{69}
\gdef \@abspage@last{80}
